{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (2.2.1)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (2024.10.0)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.12/site-packages (0.30.2)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub) (4.13.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub)\n",
      "  Using cached hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
      "Using cached hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "Installing collected packages: hf-xet, fsspec, dill, multiprocess, huggingface_hub, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\n",
      "\u001b[2K    Found existing installation: fsspec 2024.10.0\n",
      "\u001b[2K    Uninstalling fsspec-2024.10.0:\n",
      "\u001b[2K      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[2K  Attempting uninstall: dill\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: dill 0.4.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.4.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: multiprocess[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.18━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.18:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.18━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: huggingface_hub 0.30.2━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling huggingface_hub-0.30.2:90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-0.30.2━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: datasets━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Found existing installation: datasets 2.2.10m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Uninstalling datasets-2.2.1:0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [huggingface_hub]\n",
      "\u001b[2K      Successfully uninstalled datasets-2.2.1[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [huggingface_hub]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [datasets]5/6\u001b[0m [datasets]ub]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.3.0 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "jupyter-ai 2.31.4 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
      "autogluon-multimodal 1.3.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.51.3 which is incompatible.\n",
      "autogluon-timeseries 1.3.0 requires coreforecast<0.0.16,>=0.0.12, but you have coreforecast 0.0.16 which is incompatible.\n",
      "autogluon-timeseries 1.3.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.51.3 which is incompatible.\n",
      "jupyter-scheduler 2.10.0 requires fsspec<=2024.10.0,>=2023.6.0, but you have fsspec 2025.3.0 which is incompatible.\n",
      "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\n",
      "s3fs 2024.10.0 requires fsspec==2024.10.0.*, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 hf-xet-1.1.2 huggingface_hub-0.32.3 multiprocess-0.70.16\n",
      "Collecting vllm\n",
      "  Using cached vllm-0.9.0.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.12/site-packages (from vllm) (2024.11.6)\n",
      "Requirement already satisfied: cachetools in /opt/conda/lib/python3.12/site-packages (from vllm) (5.5.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from vllm) (5.9.8)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from vllm) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from vllm) (4.67.1)\n",
      "Collecting blake3 (from vllm)\n",
      "  Using cached blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: transformers>=4.51.1 in /opt/conda/lib/python3.12/site-packages (from vllm) (4.51.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.32.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub[hf_xet]>=0.32.0->vllm) (0.32.3)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /opt/conda/lib/python3.12/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from vllm) (5.28.3)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.12)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from vllm) (3.9.5)\n",
      "Collecting openai>=1.52.0 (from vllm)\n",
      "  Downloading openai-1.83.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic>=2.9 in /opt/conda/lib/python3.12/site-packages (from vllm) (2.11.4)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from vllm) (11.2.1)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
      "  Using cached lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
      "  Using cached llguidance-0.7.26-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting outlines==0.1.11 (from vllm)\n",
      "  Using cached outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.19 (from vllm)\n",
      "  Using cached xgrammar-0.1.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /opt/conda/lib/python3.12/site-packages (from vllm) (4.13.2)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /opt/conda/lib/python3.12/site-packages (from vllm) (3.18.0)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (26.4.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf>=0.13.0 (from vllm)\n",
      "  Using cached gguf-0.17.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
      "  Using cached mistral_common-1.5.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Collecting setuptools<80,>=77.0.3 (from vllm)\n",
      "  Using cached setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting einops (from vllm)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.9.4 (from vllm)\n",
      "  Using cached compressed_tensors-0.9.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.18.0 (from vllm)\n",
      "  Using cached depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.12/site-packages (from vllm) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in /opt/conda/lib/python3.12/site-packages (from vllm) (1.0.5)\n",
      "Requirement already satisfied: python-json-logger in /opt/conda/lib/python3.12/site-packages (from vllm) (2.0.7)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from vllm) (1.15.2)\n",
      "Collecting ninja (from vllm)\n",
      "  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from vllm) (1.32.1)\n",
      "Collecting opentelemetry-exporter-otlp>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_exporter_otlp-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions-ai>=0.4.1 (from vllm)\n",
      "  Using cached opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: numba==0.61.2 in /opt/conda/lib/python3.12/site-packages (from vllm) (0.61.2)\n",
      "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached ray-2.46.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting torch==2.7.0 (from vllm)\n",
      "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchaudio==2.7.0 (from vllm)\n",
      "  Using cached torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchvision==0.22.0 (from vllm)\n",
      "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting xformers==0.0.30 (from vllm)\n",
      "  Using cached xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting astor (from depyf==0.18.0->vllm)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.12/site-packages (from depyf==0.18.0->vllm) (0.3.8)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.12/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Collecting interegular (from outlines==0.1.11->vllm)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Requirement already satisfied: diskcache in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
      "Requirement already satisfied: referencing in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
      "  Using cached airportsdata-20250523-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
      "  Using cached outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->vllm) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->vllm) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->vllm) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->vllm)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0->vllm)\n",
      "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (24.2)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.9->vllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.9->vllm) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (1.3.1)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.5 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.3)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /opt/conda/lib/python3.12/site-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.11.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm) (1.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.52.0->vllm)\n",
      "  Using cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.26.0->vllm) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api>=1.26.0->vllm) (6.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.26.0->vllm) (3.21.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.26.0->vllm) (1.17.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.33.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.33.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.67.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-api>=1.26.0 (from vllm)\n",
      "  Using cached opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk>=1.26.0->vllm)\n",
      "  Using cached opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (1.26.19)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached cupy_cuda12x-13.4.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: rich>=13.7.1 in /opt/conda/lib/python3.12/site-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->vllm) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->vllm) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->vllm) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->vllm) (1.20.0)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp->vllm) (0.3.1)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
      "  Using cached fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Using cached vllm-0.9.0.1-cp38-abi3-manylinux1_x86_64.whl (377.2 MB)\n",
      "Using cached compressed_tensors-0.9.4-py3-none-any.whl (100 kB)\n",
      "Using cached depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Using cached outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "Using cached outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "Using cached xformers-0.0.30-cp312-cp312-manylinux_2_28_x86_64.whl (31.5 MB)\n",
      "Using cached xgrammar-0.1.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "Using cached llguidance-0.7.26-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.4 MB)\n",
      "Using cached lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "Using cached setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
      "Using cached gguf-0.17.0-py3-none-any.whl (95 kB)\n",
      "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Using cached mistral_common-1.5.6-py3-none-any.whl (6.5 MB)\n",
      "Downloading openai-1.83.0-py3-none-any.whl (723 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m723.4/723.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "Using cached opentelemetry_exporter_otlp-1.33.1-py3-none-any.whl (7.0 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.33.1-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
      "Using cached opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
      "Using cached opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl (5.6 kB)\n",
      "Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached ray-2.46.0-cp312-cp312-manylinux2014_x86_64.whl (68.5 MB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached airportsdata-20250523-py3-none-any.whl (912 kB)\n",
      "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached blake3-1.0.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "Using cached cupy_cuda12x-13.4.1-cp312-cp312-manylinux2014_x86_64.whl (105.3 MB)\n",
      "Using cached fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "Using cached partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: py-cpuinfo, nvidia-cusparselt-cu12, fastrlock, blake3, setuptools, pycountry, partial-json-parser, opentelemetry-semantic-conventions-ai, opentelemetry-proto, opencv-python-headless, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, msgspec, llguidance, lark, jiter, interegular, gguf, einops, cupy-cuda12x, astor, airportsdata, triton, tiktoken, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, depyf, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, openai, nvidia-cusolver-cu12, lm-format-enforcer, torch, ray, outlines_core, opentelemetry-sdk, mistral_common, xgrammar, xformers, torchvision, torchaudio, outlines, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, compressed-tensors, opentelemetry-exporter-otlp, vllm\n",
      "\u001b[2K  Attempting uninstall: setuptools━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/58\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: setuptools 80.1.0━━━\u001b[0m \u001b[32m 1/58\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling setuptools-80.1.0:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/58\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled setuptools-80.1.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/58\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-apim━━━━━━━━━━━━\u001b[0m \u001b[32m32/58\u001b[0m [opentelemetry-exporter-otlp-proto-common]\n",
      "\u001b[2K    Found existing installation: opentelemetry-api 1.32.132m32/58\u001b[0m [opentelemetry-exporter-otlp-proto-common]\n",
      "\u001b[2K    Uninstalling opentelemetry-api-1.32.1:━━━━━━━━\u001b[0m \u001b[32m32/58\u001b[0m [opentelemetry-exporter-otlp-proto-common]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-api-1.32.1\u001b[32m32/58\u001b[0m [opentelemetry-exporter-otlp-proto-common]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-semantic-conventions━━━━━━━━\u001b[0m \u001b[32m36/58\u001b[0m [nvidia-cudnn-cu12]12]mon]\n",
      "\u001b[2K    Found existing installation: opentelemetry-semantic-conventions 0.53b1/58\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling opentelemetry-semantic-conventions-0.53b1:━━━\u001b[0m \u001b[32m36/58\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-semantic-conventions-0.53b136/58\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m41/58\u001b[0m [nvidia-cusolver-cu12]entions]\n",
      "\u001b[2K    Found existing installation: torch 2.6.00m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m41/58\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.6.0:━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m43/58\u001b[0m [torch]olver-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.6.0╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m43/58\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: ray━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m43/58\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: ray 2.44.1\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m43/58\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling ray-2.44.1:━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m43/58\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled ray-2.44.1m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m44/58\u001b[0m [ray]\n",
      "\u001b[2K  Attempting uninstall: opentelemetry-sdk0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m45/58\u001b[0m [outlines_core]\n",
      "\u001b[2K    Found existing installation: opentelemetry-sdk 1.32.1━━━━━\u001b[0m \u001b[32m45/58\u001b[0m [outlines_core]\n",
      "\u001b[2K    Uninstalling opentelemetry-sdk-1.32.1:0m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m45/58\u001b[0m [outlines_core]\n",
      "\u001b[2K      Successfully uninstalled opentelemetry-sdk-1.32.1━━━━━━━\u001b[0m \u001b[32m45/58\u001b[0m [outlines_core]\n",
      "\u001b[2K  Attempting uninstall: torchvision━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m49/58\u001b[0m [xformers]ommon]\n",
      "\u001b[2K    Found existing installation: torchvision 0.21.0\u001b[90m━━━━━━\u001b[0m \u001b[32m49/58\u001b[0m [xformers]\n",
      "\u001b[2K    Uninstalling torchvision-0.21.0:━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m49/58\u001b[0m [xformers]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.21.00m\u001b[90m━━━━━━\u001b[0m \u001b[32m49/58\u001b[0m [xformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58/58\u001b[0m [vllm]0m [vllm]0m [outlines]o]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.3.0 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "dash 2.18.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-table==5.0.0, which is not installed.\n",
      "autogluon-multimodal 1.3.0 requires torch<2.7,>=2.2, but you have torch 2.7.0 which is incompatible.\n",
      "autogluon-multimodal 1.3.0 requires torchvision<0.22.0,>=0.16.0, but you have torchvision 0.22.0 which is incompatible.\n",
      "autogluon-multimodal 1.3.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.51.3 which is incompatible.\n",
      "autogluon-timeseries 1.3.0 requires coreforecast<0.0.16,>=0.0.12, but you have coreforecast 0.0.16 which is incompatible.\n",
      "autogluon-timeseries 1.3.0 requires torch<2.7,>=2.2, but you have torch 2.7.0 which is incompatible.\n",
      "autogluon-timeseries 1.3.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.51.3 which is incompatible.\n",
      "dash 2.18.1 requires Flask<3.1,>=1.0.4, but you have flask 3.1.0 which is incompatible.\n",
      "dash 2.18.1 requires Werkzeug<3.1, but you have werkzeug 3.1.3 which is incompatible.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
      "spacy 3.8.5 requires thinc<8.4.0,>=8.3.4, but you have thinc 8.3.2 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed airportsdata-20250523 astor-0.8.1 blake3-1.0.5 compressed-tensors-0.9.4 cupy-cuda12x-13.4.1 depyf-0.18.0 einops-0.8.1 fastrlock-0.8.3 gguf-0.17.0 interegular-0.3.3 jiter-0.10.0 lark-1.2.2 llguidance-0.7.26 lm-format-enforcer-0.10.11 mistral_common-1.5.6 msgspec-0.19.0 ninja-1.11.1.4 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 openai-1.83.0 opencv-python-headless-4.11.0.86 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-grpc-1.33.1 opentelemetry-exporter-otlp-proto-http-1.33.1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 opentelemetry-semantic-conventions-ai-0.4.9 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 py-cpuinfo-9.0.0 pycountry-24.6.1 ray-2.46.0 setuptools-79.0.1 tiktoken-0.9.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0 vllm-0.9.0.1 xformers-0.0.30 xgrammar-0.1.19\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets fsspec huggingface_hub\n",
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "# #dataset = load_dataset('squad', split='train')\n",
    "\n",
    "\n",
    "#ds = load_dataset(\"openai/gsm8k\", \"socratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7473"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds[\"train\"][\"question\"])\n",
    "len(ds[\"train\"][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n",
      "72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7473"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(ds[\"train\"][\"answer\"][0])\n",
    "# print(ds[\"train\"][\"answer\"][0].split('\\n#### ')[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def process_ground_truth(ans):\n",
    "#     return ans.split('\\n#### ')[1]\n",
    "\n",
    "# with ProcessPoolExecutor() as executor:\n",
    "#     ground_truth = list(executor.map(process_ground_truth, ds[\"train\"][\"answer\"]))\n",
    "# len(ground_truth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\\nUser: {question}\\nAssistant: <think>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from vllm import LLM, SamplingParams\n",
    "from drgrpo_grader import r1_zero_reward_fn\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "def preprocess_dataset(ds):\n",
    "    questions, answers =  ds[\"train\"][\"question\"], ds[\"train\"][\"answer\"]\n",
    "    with open(\"prompts/r1_zero.prompt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        prompt_string = f.read()\n",
    "    def process_question(q):\n",
    "        return prompt_string.format(question=q)\n",
    "    def process_ground_truth(ans):\n",
    "    return ans.split('\\n#### ')[1]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        question_prompts = list(executor.map(process_question, ds[\"train\"][\"question\"]))\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        ground_truth = list(executor.map(process_ground_truth, ds[\"train\"][\"answer\"]))\n",
    "    return question_prompts, ground_truth\n",
    "\n",
    "\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "question_prompts, ground_truth = preprocess_dataset(ds)\n",
    "\n",
    "\n",
    "# Create a sampling params object, stopping generation on newline.\n",
    "sampling_params = SamplingParams(\n",
    "temperature=1.0, top_p=1.0, max_tokens=1024, stop=[\"\\n\"]\n",
    ")\n",
    "# Create an LLM.\n",
    "\n",
    "llm = LLM(model=\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "model_outputs = llm.generate(question_prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "final_output = []\n",
    "for output, gt in zip(model_outputs, ground_truth):\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "    res = r1_zero_reward_fn(generated_text, gt)\n",
    "    \n",
    "    final_output.append([res['format_reward'], res['answer_reward'], res['reward']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-02 05:22:20 [__init__.py:247] No platform detected, vLLM is running on UnspecifiedPlatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 05:22:21.462756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-02 05:22:26 [_custom_ops.py:21] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n",
      "INFO 06-02 05:22:28 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-02 05:22:28 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-02 05:22:28 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Device string must not be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     11\u001b[0m temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create an LLM.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen2.5-1.5B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Generate texts from the prompts. The output is a list of RequestOutput objects\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# that contain the prompt, generated text, and other information.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(prompts, sampling_params)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/vllm/utils.py:1183\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1178\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1179\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1180\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         )\n\u001b[0;32m-> 1183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/vllm/entrypoints/llm.py:253\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    224\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    225\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/vllm/engine/llm_engine.py:494\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m vllm_config \u001b[38;5;241m=\u001b[39m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m engine_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m envs\u001b[38;5;241m.\u001b[39mVLLM_USE_V1:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/vllm/engine/arg_utils.py:982\u001b[0m, in \u001b[0;36mEngineArgs.create_engine_config\u001b[0;34m(self, usage_context)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[1;32m    980\u001b[0m current_platform\u001b[38;5;241m.\u001b[39mpre_register_and_update()\n\u001b[0;32m--> 982\u001b[0m device_config \u001b[38;5;241m=\u001b[39m \u001b[43mDeviceConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_platform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model_config()\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# * If VLLM_USE_V1 is unset, we enable V1 for \"supported features\"\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m#   and fall back to V0 for experimental or unsupported features.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# * If VLLM_USE_V1=1, we enable V1 for supported + experimental\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m#   features and raise error for unsupported features.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# * If VLLM_USE_V1=0, we disable V1.\u001b[39;00m\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, device)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/vllm/config.py:2260\u001b[0m, in \u001b[0;36mDeviceConfig.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# Set device with device type\u001b[39;00m\n\u001b[0;32m-> 2260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Device string must not be empty"
     ]
    }
   ],
   "source": [
    "\n",
    "#r1_zero_reward_fn(response, ground_truth, fast=True)\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "\"Hello, my name is\",\n",
    "\"The president of the United States is\",\n",
    "\"The capital of France is\",\n",
    "\"The future of AI is\",\n",
    "]\n",
    "# Create a sampling params object, stopping generation on newline.\n",
    "sampling_params = SamplingParams(\n",
    "temperature=1.0, top_p=1.0, max_tokens=1024, stop=[\"\\n\"]\n",
    ")\n",
    "# Create an LLM.\n",
    "\n",
    "llm = LLM(model=\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(ds[\"train\"][\"question\"], sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
