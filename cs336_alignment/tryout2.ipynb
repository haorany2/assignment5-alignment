{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def preprocess_dataset(ds):\n",
    "    questions, answers =  ds[\"train\"][\"question\"], ds[\"train\"][\"answer\"]\n",
    "    with open(\"prompts/r1_zero.prompt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        prompt_string = f.read()\n",
    "\n",
    "    def process_question(q):\n",
    "        return prompt_string.format(question=q)\n",
    "    def process_ground_truth(ans):\n",
    "        return ans.split('\\n#### ')[1]\n",
    "    def process_prompt_completion(q, ans):\n",
    "        prompt = prompt_string.format(question=q)\n",
    "        cot =' ' + ans.split('\\n#### ')[0] + ' </think>'\n",
    "        gt = f\" <answer> {ans.split('\\n#### ')[1]} </answer>\"\n",
    "        return prompt + cot + gt\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        question_prompts = list(executor.map(process_question, ds[\"train\"][\"question\"]))\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        ground_truth = list(executor.map(process_ground_truth, ds[\"train\"][\"answer\"]))\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        prompt_completion = list(executor.map(process_prompt_completion, ds[\"train\"][\"question\"], ds[\"train\"][\"answer\"]))\n",
    "    return question_prompts, ground_truth, prompt_completion\n",
    "\n",
    "\n",
    "# Build a collator whose response_template matches your prompt ending\n",
    "# by default, truncate from left side, and sacrifice prompt\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer = tokenizer,\n",
    "    # Anything before *and including* this string gets label = -100\n",
    "    response_template  = r\"Assistant: <think>\",   # note the space after >\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "#preprocess_dataset(ds)[2][0:2]\n",
    "\n",
    "# some test on collator\n",
    "# curr_batch = tokenizer(\n",
    "#     preprocess_dataset(ds)[2][0:2],\n",
    "#     padding=False,               # collator can handle this , set to false\n",
    "#     truncation=False,           # collator will handle this with left truncate\n",
    "#     return_special_tokens_mask=False   # only need for MLM task\n",
    "#     #return_tensors=None        # lists, not tensors – collator wants lists\n",
    "# )\n",
    "\n",
    "# #collator format [[dict] , [dict], [dict]]\n",
    "# cnt = len(curr_batch['input_ids'])\n",
    "\n",
    "# curr_batch_reconstruct =[]\n",
    "# for i in range(cnt):\n",
    "#     temp = dict()\n",
    "#     for k,vals in curr_batch.items():\n",
    "#         temp[k] = vals[i]\n",
    "#     curr_batch_reconstruct.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# collator_output_dict = collator.torch_call(curr_batch_reconstruct)\n",
    "# collator_output_dict['labels'][1]\n",
    "# #print(collator_output_dict['input_ids'][0][128:])\n",
    "# #print(collator_output_dict['labels'][0][128:])\n",
    "# # print(tokenizer.decode(collator_output_dict['input_ids'][1]))\n",
    "# # print(tokenizer.decode([42 if v==-100 else v for v in collator_output_dict['labels'][1].tolist() ]))\n",
    "\n",
    "\n",
    "# #collator([12,33])\n",
    "# #print(curr_batch.keys())\n",
    "# #len(curr_batch['input_ids'][0]), len(curr_batch['input_ids'][1])    \n",
    "# #curr_batch['attention_mask'][1] # the second mask has 1 digit 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     ).to(policy_device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# Make sure your tokenizer knows about <|im_end|> already:\n",
    "#   (Qwen’s tokenizer has <|im_end|> in additional_special_tokens, dont need to create id, only find it through convert_tokens_to_ids)\n",
    "eos_token_str = \"<|im_end|>\"\n",
    "eos_id = tokenizer.convert_tokens_to_ids(eos_token_str) # only for existed pair mapping\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    max_seq_length=1024,\n",
    "    pad_token_id=eos_id,\n",
    "    eos_token_id=eos_id,          # <— this is what TRL will use to stop\n",
    "    # you can also set other generation defaults here if you like\n",
    ")\n",
    "\n",
    "\n",
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,  \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# 将LoRA配置应用到模型\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 训练参数配置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft_lora_results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,  \n",
    "    num_train_epochs=3,\n",
    "    fp16=True,  \n",
    "    logging_steps=1\n",
    ")\n",
    "\n",
    "# 使用Trainer API进行训练\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=SFTDataset(\"SFT_data.json\"),\n",
    "    data_collator=torch.utils.data.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,  11657,   6236,   6331,\n",
       "            879,   2677,  30580,    304,    279,   1707,    315,    264,  53966,\n",
       "         151645,    198, 151644,    872,    198,   4340,   1657,  58332,    646,\n",
       "            264,   3738,   8180,    304,    825,  11699,     30, 151645,    198,\n",
       "         151644,  77091,    198]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer.decode(tokenizer.pad_token_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "tokenized_chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a friendly chatbot who always responds in the style of a pirate<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_chat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']\n"
     ]
    }
   ],
   "source": [
    "#setup_chat_format is used to create chat template with [{role:xx, content:xx}] data, to convert it to purely text\n",
    "#we dont need to setup_chat_format for qwen. it has been setup\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from trl import SFTConfig, SFTTrainer\n",
    "# from trl import setup_chat_format\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "# tokenizer.decode(tokenizer.eos_token_id)\n",
    "print(tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
