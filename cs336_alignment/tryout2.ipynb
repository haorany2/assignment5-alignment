{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 20:00:54 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast, PreTrainedModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset, Dataset\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor import set_random_seed as vllm_set_random_seed\n",
    "from drgrpo_grader import r1_zero_reward_fn\n",
    "import gc\n",
    "from unittest.mock import patch\n",
    "import wandb\n",
    "import safetensors\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    ).to(device)\n",
    "tokenizer.padding_side = \"left\"\n",
    "# 1) Add a new [PAD] token\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "# 2) Resize the model’s embeddings to match the new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# by default, truncate from left side, and sacrifice prompt\n",
    "# Make sure your tokenizer knows about <|im_end|> already:\n",
    "#   (Qwen’s tokenizer has <|im_end|> in additional_special_tokens, dont need to create id, only find it through convert_tokens_to_ids)\n",
    "\n",
    "# 1) Look up the ID of the built-in <|im_end|> token:\n",
    "im_end_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "\n",
    "# 2) Tell the tokenizer to use that as its pad token:\n",
    "#tokenizer.pad_token = \"<|im_end|>\"\n",
    "tokenizer.eos_token_id = im_end_id\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def preprocess_dataset(ds, usage):\n",
    "    questions, answers =  ds[usage][\"question\"], ds[usage][\"answer\"]\n",
    "    with open(\"prompts/r1_zero.prompt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        prompt_string = f.read()\n",
    "\n",
    "    def process_question(q):\n",
    "        return prompt_string.format(question=q)\n",
    "    def process_ground_truth(ans):\n",
    "        return ans.split('\\n#### ')[1]\n",
    "    def process_prompt_completion(q, ans):\n",
    "        prompt = prompt_string.format(question=q)\n",
    "        cot =' ' + ans.split('\\n#### ')[0] + ' </think>'\n",
    "        gt = f\" <answer> {ans.split('\\n#### ')[1]} </answer>\"\n",
    "        return prompt + cot + gt\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        question_prompts = list(executor.map(process_question, ds[usage][\"question\"]))\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        ground_truth = list(executor.map(process_ground_truth, ds[usage][\"answer\"]))\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        prompt_completion = list(executor.map(process_prompt_completion, ds[usage][\"question\"], ds[usage][\"answer\"]))\n",
    "    return question_prompts, ground_truth, prompt_completion\n",
    "\n",
    "training_data = preprocess_dataset(ds, 'train')[2]\n",
    "test_prompt, test_gt =  preprocess_dataset(ds, 'test')[0], preprocess_dataset(ds, 'test')[1]\n",
    "train_ds = Dataset.from_dict({\"text\": preprocess_dataset(ds, 'train')[2]})\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"prompt\": test_prompt[:len(test_prompt)//2],\n",
    "    \"gt\": test_gt[:len(test_gt)//2],\n",
    "\n",
    "})\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"prompt\": test_prompt[len(test_prompt)//2:],\n",
    "    \"gt\": test_gt[len(test_gt)//2:],\n",
    "\n",
    "})\n",
    "# val_ds = Dataset.from_dict({\n",
    "#     \"prompt\": test_prompt,\n",
    "#     \"gt\": test_gt,\n",
    "\n",
    "# })\n",
    "\n",
    "\n",
    "# Build a collator whose response_template matches your prompt ending\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer = tokenizer,\n",
    "    # Anything before *and including* this string gets label = -100\n",
    "    response_template  = r\"Assistant: <think>\",   # note the space after >\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "#preprocess_dataset(ds)[2][0:2]\n",
    "\n",
    "# some test on collator\n",
    "# curr_batch = tokenizer(\n",
    "#     preprocess_dataset(ds)[2][0:2],\n",
    "#     padding=False,               # collator can handle this , set to false\n",
    "#     truncation=False,           # collator will handle this with left truncate\n",
    "#     return_special_tokens_mask=False   # only need for MLM task\n",
    "#     #return_tensors=None        # lists, not tensors – collator wants lists\n",
    "# )\n",
    "\n",
    "# # #collator format [[dict] , [dict], [dict]]\n",
    "# cnt = len(curr_batch['input_ids'])\n",
    "\n",
    "# curr_batch_reconstruct =[]\n",
    "# for i in range(cnt):\n",
    "#     temp = dict()\n",
    "#     for k,vals in curr_batch.items():\n",
    "#         temp[k] = vals[i]\n",
    "#     curr_batch_reconstruct.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# collator_output_dict = collator.torch_call(curr_batch_reconstruct)\n",
    "# collator_output_dict['labels'][1]\n",
    "# print(collator_output_dict['input_ids'][0][128:])\n",
    "# print(collator_output_dict['labels'][0][128:])\n",
    "# print(len(collator_output_dict['input_ids'][0]))\n",
    "# print(len(collator_output_dict['labels'][0]))\n",
    "# # # print(tokenizer.decode(collator_output_dict['input_ids'][1]))\n",
    "# # # print(tokenizer.decode([42 if v==-100 else v for v in collator_output_dict['labels'][1].tolist() ]))\n",
    "\n",
    "\n",
    "# # #collator([12,33])\n",
    "# # #print(curr_batch.keys())\n",
    "# # #len(curr_batch['input_ids'][0]), len(curr_batch['input_ids'][1])    \n",
    "# # #curr_batch['attention_mask'][1] # the second mask has 1 digit 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\\nUser: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\\nAssistant: <think>\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.decode([151645])\n",
    "#train_ds['text']\n",
    "tokenizer.pad_token_id\n",
    "#tokenizer.eos_token_id\n",
    "#val_ds['prompt']\n",
    "# print(training_data[0])\n",
    "#print(val_ds['prompt'][0])\n",
    "# print(ds['train']['question'][0])\n",
    "# print(ds['test']['question'][0])\n",
    "test_prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca7707368d34a3f82877b9c8d1be9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13993a9fc54145baae39dacd28e998d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5793bd02f3a468388d6938b742859b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28be7250b9f4ad38bc12ed47898e002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhaoranyu66\u001b[0m (\u001b[33mudacity_jeff\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sagemaker-user/assignment5-alignment/cs336_alignment/wandb/run-20250608_200600-iaqfilz1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/udacity_jeff/huggingface/runs/iaqfilz1' target=\"_blank\">./checkpoint/sft/sft_lora_results3</a></strong> to <a href='https://wandb.ai/udacity_jeff/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/udacity_jeff/huggingface' target=\"_blank\">https://wandb.ai/udacity_jeff/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/udacity_jeff/huggingface/runs/iaqfilz1' target=\"_blank\">https://wandb.ai/udacity_jeff/huggingface/runs/iaqfilz1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='838' max='2805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 838/2805 15:17 < 35:58, 0.91 it/s, Epoch 0.90/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.528500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.560500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.528400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.487600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.508600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.484800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.389700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.365100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.362700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.348300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.332700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.374900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.344100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ca1c2076-3042-40cc-b816-9f45fa93574c)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2.5-1.5B.\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.conda/envs/myenv/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen2.5-1.5B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    max_seq_length=1024,\n",
    "    output_dir=\"./checkpoint/sft/sft_lora_results3\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-6,  \n",
    "    num_train_epochs=3,\n",
    "   \n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"],\n",
    "    warmup_ratio=0.05,\n",
    "    report_to = \"wandb\",  \n",
    "    bf16=True,   \n",
    "    #pad_token_id=eos_id,\n",
    "    #eos_token_id=eos_id,          # <— this is what TRL will use to stop\n",
    "    # you can also set other generation defaults here if you like\n",
    ")\n",
    "\n",
    "\n",
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,  \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "   \n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    "    target_modules='all-linear'\n",
    "    #target_modules = [\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "# 将LoRA配置应用到模型\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 训练参数配置\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./checkpoint/sft/sft_lora_results\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     learning_rate=1e-4,  \n",
    "#     num_train_epochs=3,\n",
    "#     fp16=True,  \n",
    "#     logging_steps=1\n",
    "# )\n",
    "\n",
    "# 使用Trainer API进行训练\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    #args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collator, \n",
    "    args=sft_config,\n",
    "    #data_collator=torch.utils.data.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_vllm(model_id: str, tokenizer, device: str, seed: int, gpu_memory_utilization: float = 0.85):\n",
    "    \"\"\"Start the inference process, here we use vLLM to hold a model on\n",
    "    a GPU separate from the policy.\n",
    "    \"\"\"\n",
    "    vllm_set_random_seed(seed)\n",
    "    # Monkeypatch from TRL:\n",
    "    # https://github.com/huggingface/trl/blob/\n",
    "    # 22759c820867c8659d00082ba8cf004e963873c1/trl/trainer/grpo_trainer.py\n",
    "    # Patch vLLM to make sure we can\n",
    "    # (1) place the vLLM model on the desired device (world_size_patch) and\n",
    "    # (2) avoid a test that is not designed for our setting (profiling_patch).\n",
    "    world_size_patch = patch(\"torch.distributed.get_world_size\", return_value=1)\n",
    "    profiling_patch = patch(\n",
    "    \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n",
    "    return_value=None\n",
    "    )\n",
    "    with world_size_patch, profiling_patch:\n",
    "        return LLM(\n",
    "        model=model_id,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_mode='auto',\n",
    "        device=device,\n",
    "        dtype=torch.bfloat16,\n",
    "        enable_prefix_caching=True,\n",
    "        gpu_memory_utilization=gpu_memory_utilization,\n",
    "        \n",
    "        )\n",
    "\n",
    "def load_policy_into_vllm_instance(policy: PreTrainedModel, tokenizer, llm: LLM):\n",
    "    \"\"\" Copied from https://github.com/huggingface/trl/blob/\n",
    "    22759c820867c8659d00082ba8cf004e963873c1/trl/trainer/grpo_trainer.py#L670.\n",
    "    \"\"\"\n",
    "    state_dict = policy.state_dict()\n",
    "    llm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model\n",
    "    llm_model.load_weights(state_dict.items())\n",
    "    #llm.llm_engine.tokenizer = tokenizer\n",
    "\n",
    "def sft_evaluation(policy: PreTrainedModel, tokenizer, val_ds, model_id: str, policy_device: str, llm_device: str, out_dir):\n",
    "    start_time = time.time()\n",
    "    #format_reward, answer_reward, answer = [], [], []\n",
    "    # total_response_len = 0\n",
    "    # total_response_len_correct = 0\n",
    "    # total_response_len_incorrect = 0\n",
    "    # total_sample = 0\n",
    "    # total_correct_sample = 0\n",
    "    # total_incorrect_sample = 0\n",
    "    response_avg_entropy_lst = [] #(batch,)\n",
    "    response_len_lst = [] #(batch,)\n",
    "    correct_lst = []\n",
    "    incorrect_lst = []\n",
    "    #initialize llm for vllm\n",
    "    llm = init_vllm(model_id, tokenizer, llm_device, 233)\n",
    "    load_policy_into_vllm_instance(policy, tokenizer, llm)\n",
    "    sampling_params = SamplingParams(\n",
    "    temperature=1.0, top_p=1.0, max_tokens=1024, stop=[\"</answer>\"]\n",
    "    )\n",
    "    sampling_params.include_stop_str_in_output = True\n",
    "    all_prompt_texts = val_ds['prompt']\n",
    "    all_answer_gt = val_ds['gt']\n",
    "    # for batch in val_dataset:\n",
    "    #     all_prompt_texts.extend(batch['prompt_texts'])\n",
    "    #     all_answer_gt.extend(batch['answer_gt'] ) \n",
    "   \n",
    "    all_model_outputs = llm.generate(all_prompt_texts, sampling_params)\n",
    "    start2_time = time.time()\n",
    "    print(f'generate all response time: {start2_time - start_time}------------')\n",
    "\n",
    "    # still need policy model for eval mode\n",
    "    #policy.eval()\n",
    "    os.makedirs(out_dir.rsplit('/', 1)[0], exist_ok=True) \n",
    "    with open(out_dir, \"a\", encoding=\"utf-8\") as f:\n",
    "        # for idx, batch in enumerate(tqdm(dataloader, total=len(dataloader))):\n",
    "        #     #start2_time = time.time()\n",
    "        #     #prompt_texts = batch['prompt_texts']\n",
    "        #     #answer_texts = batch['answer_texts'] \n",
    "        #     #answer_gt = batch['answer_gt'] \n",
    "            \n",
    "        #     #batch_size = batch['input_ids'].size(0)\n",
    "        #     input_ids = batch['input_ids'].to(policy_device)\n",
    "        #     labels = batch['labels'].to(policy_device)\n",
    "        #     response_masks = batch['response_mask'].to(policy_device)\n",
    "        #     with torch.no_grad():\n",
    "        #         response = get_response_log_probs(policy, input_ids, labels, True)\n",
    "        #         log_probs, token_entropy = response['log_probs'], response['token_entropy']\n",
    "        #         normalzied_token_entropy = token_entropy * response_masks\n",
    "                \n",
    "        #         curr_batch_response_len = torch.sum(response_masks,dim=1)#.tolist()\n",
    "        #         curr_batch_response_entropy = torch.sum(normalzied_token_entropy, dim=1)#.tolist()\n",
    "        #         curr_batch_response_avg_entropy = curr_batch_response_entropy / curr_batch_response_len\n",
    "                \n",
    "\n",
    "        #         curr_batch_response_len = curr_batch_response_len.tolist()\n",
    "        #         curr_batch_response_avg_entropy = curr_batch_response_avg_entropy.tolist()\n",
    "        #         response_len_lst.extend(curr_batch_response_len)\n",
    "        #         response_avg_entropy_lst.extend(curr_batch_response_avg_entropy)\n",
    "        #         # print('curr_batch_response_len', curr_batch_response_len)\n",
    "        #         # print('curr_batch_response_avg_entropy', curr_batch_response_avg_entropy)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        #     # # Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "        #     # # that contain the prompt, generated text, and other information.\n",
    "        #     # stop1_time = time.time()\n",
    "        #     # model_outputs = llm.generate(prompt_texts, sampling_params)\n",
    "        #     # stop2_time = time.time()\n",
    "\n",
    "        for i, (output, gt) in enumerate(zip(all_model_outputs, all_answer_gt)):\n",
    "            prompt = output.prompt\n",
    "            generated_text = output.outputs[0].text\n",
    "            \n",
    "            res = r1_zero_reward_fn(generated_text, gt)\n",
    "            # format_reward+= res['format_reward']\n",
    "            # answer_reward+= res['answer_reward']\n",
    "            # reward+= res['reward']\n",
    "            #print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}, format_reward: {str(res['format_reward'])}, answer_reward: {str(res['answer_reward'])}, reward: {str(res['reward'])}\")\n",
    "            #final_output.append([res['format_reward'], res['answer_reward'], res['reward']])\n",
    "            dp = {\n",
    "                \"prompt\": f\"{prompt}\",\n",
    "                \"ground_truth\": gt, \n",
    "                \"output\": f\"{generated_text}\",\n",
    "                \"format_reward\": res['format_reward'],\n",
    "                \"answer_reward\": res['answer_reward'],\n",
    "                \"reward\": res['reward'],\n",
    "                #\"avg_response_token_entropy\": response_avg_entropy_lst[i]\n",
    "            }\n",
    "            correct_lst.append(int(res['reward']==1))\n",
    "            incorrect_lst.append(int(res['reward']!=1))\n",
    "\n",
    "            json.dump(dp, f, ensure_ascii=False)\n",
    "            \n",
    "    #total_response_len_correct = np.sum(np.array(correct_lst) * np.array(response_len_lst))\n",
    "    #total_response_len_incorrect = np.sum(np.array(incorrect_lst) * np.array(response_len_lst))\n",
    "    total_correct_sample = np.sum(np.array(correct_lst))\n",
    "    total_incorrect_sample = np.sum(np.array(incorrect_lst))\n",
    "    total_sample = len(all_model_outputs)\n",
    "    #total_response_len = np.sum(np.array(response_len_lst))\n",
    "\n",
    "    print(f'total_correct_sample: {total_correct_sample}, total_incorrect_sample: {total_incorrect_sample}')\n",
    "    #print(f'final stats\\navg_response_len={total_response_len/total_sample:.2f}, avg_response_len_correct={total_response_len_correct/total_correct_sample:.2f}, avg_response_len_incorrect={total_response_len_incorrect/total_incorrect_sample:.2f}')\n",
    "    #print(\"some check:\", total_response_len_correct+total_response_len_incorrect==total_response_len)\n",
    "    #print(\"moer check:\", total_correct_sample+total_incorrect_sample==total_sample)\n",
    "    import gc\n",
    "    del llm\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return total_correct_sample / total_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sft_evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     14\u001b[39m adapter_sd = safetensors.torch.load_file(\n\u001b[32m     15\u001b[39m     model_dir,\n\u001b[32m     16\u001b[39m     device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m model.load_state_dict(adapter_sd, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43msft_evaluation\u001b[49m(model, tokenizer_dir, val_ds, model_name, device, device, eval_out_dir)\n",
      "\u001b[31mNameError\u001b[39m: name 'sft_evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer_dir =  \"./checkpoint/sft/sft_lora_results2/checkpoint-1000\"\n",
    "model_dir =      \"checkpoint/sft/sft_lora_results2/checkpoint-1000/adapter_model.safetensors\"\n",
    "eval_out_dir = 'eval/sft/sft_lora_results2/checkpoint-1000/sft_eval.jsonl'\n",
    "tokenizer =  AutoTokenizer.from_pretrained(tokenizer_dir, local_files_only=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    ).to(device)\n",
    "# 2) Load only the adapter weights into the base model\n",
    "adapter_sd = safetensors.torch.load_file(\n",
    "    model_dir,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "model.load_state_dict(adapter_sd, strict=False)\n",
    "sft_evaluation(model, tokenizer_dir, val_ds, model_name, device, device, eval_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 19:42:43 config.py:542] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 06-08 19:42:43 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 06-08 19:42:43 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 06-08 19:42:43 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 06-08 19:42:44 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 06-08 19:42:45 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-1.5B...\n",
      "INFO 06-08 19:42:45 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 06-08 19:42:45 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864658fb46d74523891a8a935ecc8533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 19:42:46 model_runner.py:1115] Loading model weights took 2.9105 GB\n",
      "INFO 06-08 19:42:46 worker.py:267] Memory profiling takes 0.58 seconds\n",
      "INFO 06-08 19:42:46 worker.py:267] the current vLLM instance can use total_gpu_memory (21.98GiB) x gpu_memory_utilization (0.85) = 18.68GiB\n",
      "INFO 06-08 19:42:46 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 14.32GiB.\n",
      "INFO 06-08 19:42:47 executor_base.py:110] # CUDA blocks: 33523, # CPU blocks: 9362\n",
      "INFO 06-08 19:42:47 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 4.09x\n",
      "INFO 06-08 19:42:51 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 19:43:08 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.20 GiB\n",
      "INFO 06-08 19:43:08 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 7473/7473 [04:42<00:00, 26.43it/s, est. speed input: 4027.17 toks/s, output: 4306.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate all response time: 286.51076078414917------------\n",
      "total_correct_sample: 468, total_incorrect_sample: 7005\n",
      "1848.0 468.0 468.0\n"
     ]
    }
   ],
   "source": [
    "# base model evaluation\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "eval_out_dir = 'eval/sft/sft_lora_results2/base_model/sft_eval.jsonl'\n",
    "llm = LLM(model_name, dtype=\"bfloat16\", gpu_memory_utilization=0.85)\n",
    "# Create a sampling params object, stopping generation on newline.\n",
    "sampling_params = SamplingParams(\n",
    "temperature=1.0, top_p=1.0, max_tokens=1024, stop=[\"</answer>\"]\n",
    ")\n",
    "sampling_params.include_stop_str_in_output = True\n",
    "start_time = time.time()\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "\n",
    "all_prompt_texts = val_ds['prompt']\n",
    "all_answer_gt = val_ds['gt']\n",
    "\n",
    "\n",
    "all_model_outputs = llm.generate(all_prompt_texts, sampling_params)\n",
    "start2_time = time.time()\n",
    "print(f'generate all response time: {start2_time - start_time}------------')\n",
    "correct_lst = []\n",
    "incorrect_lst = []\n",
    "# still need policy model for eval mode\n",
    "#policy.eval()\n",
    "os.makedirs(eval_out_dir.rsplit('/', 1)[0], exist_ok=True) \n",
    "with open(eval_out_dir, \"a\", encoding=\"utf-8\") as f:\n",
    "    format_reward, answer_reward, reward = 0, 0, 0\n",
    "    for i, (output, gt) in enumerate(zip(all_model_outputs, all_answer_gt)):\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        \n",
    "        res = r1_zero_reward_fn(generated_text, gt)\n",
    "        # format_reward+= res['format_reward']\n",
    "        # answer_reward+= res['answer_reward']\n",
    "        # reward+= res['reward']\n",
    "        #print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}, format_reward: {str(res['format_reward'])}, answer_reward: {str(res['answer_reward'])}, reward: {str(res['reward'])}\")\n",
    "        #final_output.append([res['format_reward'], res['answer_reward'], res['reward']])\n",
    "        dp = {\n",
    "            \"prompt\": f\"{prompt}\",\n",
    "            \"ground_truth\": gt, \n",
    "            \"output\": f\"{generated_text}\",\n",
    "            \"format_reward\": res['format_reward'],\n",
    "            \"answer_reward\": res['answer_reward'],\n",
    "            \"reward\": res['reward'],\n",
    "            #\"avg_response_token_entropy\": response_avg_entropy_lst[i]\n",
    "        }\n",
    "        correct_lst.append(int(res['reward']==1))\n",
    "        incorrect_lst.append(int(res['reward']!=1))\n",
    "        format_reward+= res['format_reward']\n",
    "        answer_reward+= res['answer_reward']\n",
    "        reward+= res['reward']\n",
    "\n",
    "        json.dump(dp, f, ensure_ascii=False)\n",
    "            \n",
    "#total_response_len_correct = np.sum(np.array(correct_lst) * np.array(response_len_lst))\n",
    "#total_response_len_incorrect = np.sum(np.array(incorrect_lst) * np.array(response_len_lst))\n",
    "total_correct_sample = np.sum(np.array(correct_lst))\n",
    "total_incorrect_sample = np.sum(np.array(incorrect_lst))\n",
    "total_sample = len(all_model_outputs)\n",
    "#total_response_len = np.sum(np.array(response_len_lst))\n",
    "\n",
    "print(f'total_correct_sample: {total_correct_sample}, total_incorrect_sample: {total_incorrect_sample}')\n",
    "print(format_reward, answer_reward, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'72'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output = \" We'll use cancellation technique to solve this problem. Natalia sold half as many clips in may as she had in April. That means she sold 12 clips in May since 48 / 2 = 24. Note that we must never cancel the first 24/, as it is 0. Our answer is 48+24 = 72 clips.</think>\\n\\n<answer> 72 </answer>\"\n",
    "# r1_zero_reward_fn(output, '72')\n",
    "\n",
    "all_prompt_texts = val_ds['prompt']\n",
    "all_prompt_texts[0]\n",
    "val_ds['gt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,  11657,   6236,   6331,\n",
       "            879,   2677,  30580,    304,    279,   1707,    315,    264,  53966,\n",
       "         151645,    198, 151644,    872,    198,   4340,   1657,  58332,    646,\n",
       "            264,   3738,   8180,    304,    825,  11699,     30, 151645,    198,\n",
       "         151644,  77091,    198]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer.decode(tokenizer.pad_token_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "tokenized_chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a friendly chatbot who always responds in the style of a pirate<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_chat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']\n"
     ]
    }
   ],
   "source": [
    "#setup_chat_format is used to create chat template with [{role:xx, content:xx}] data, to convert it to purely text\n",
    "#we dont need to setup_chat_format for qwen. it has been setup\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from trl import SFTConfig, SFTTrainer\n",
    "# from trl import setup_chat_format\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "# tokenizer.decode(tokenizer.eos_token_id)\n",
    "print(tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
