{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-09 03:05:16 [__init__.py:243] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('checkpoint/sft/tokenizer/tokenizer_config.json',\n",
       " 'checkpoint/sft/tokenizer/special_tokens_map.json',\n",
       " 'checkpoint/sft/tokenizer/chat_template.jinja',\n",
       " 'checkpoint/sft/tokenizer/vocab.json',\n",
       " 'checkpoint/sft/tokenizer/merges.txt',\n",
       " 'checkpoint/sft/tokenizer/added_tokens.json',\n",
       " 'checkpoint/sft/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast, PreTrainedModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor import set_random_seed as vllm_set_random_seed\n",
    "from drgrpo_grader import r1_zero_reward_fn\n",
    "import gc\n",
    "from unittest.mock import patch\n",
    "import wandb\n",
    "import safetensors\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# import numpy._core.multiarray\n",
    "\n",
    "# torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])\n",
    "# os.environ[\"TORCH_LOAD_WEIGHTS_ONLY\"] = \"0\"\n",
    "\n",
    "\n",
    "import time\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    ).to(device)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"./checkpoint/sft/sft_lora_results3/checkpoint-500\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     ).to(device)\n",
    "tokenizer.padding_side = \"left\"\n",
    "# 1) Add a new [PAD] token\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "# 2) Resize the model’s embeddings to match the new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# by default, truncate from left side, and sacrifice prompt\n",
    "# Make sure your tokenizer knows about <|im_end|> already:\n",
    "#   (Qwen’s tokenizer has <|im_end|> in additional_special_tokens, dont need to create id, only find it through convert_tokens_to_ids)\n",
    "\n",
    "# 1) Look up the ID of the built-in <|im_end|> token:\n",
    "im_end_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "\n",
    "# 2) Tell the tokenizer to use that as its pad token:\n",
    "#tokenizer.pad_token = \"<|im_end|>\"\n",
    "tokenizer.eos_token_id = im_end_id\n",
    "\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def preprocess_dataset(ds, usage):\n",
    "    questions, answers =  ds[usage][\"question\"], ds[usage][\"answer\"]\n",
    "    with open(\"prompts/r1_zero.prompt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        prompt_string = f.read()\n",
    "\n",
    "    def process_question(q):\n",
    "        return prompt_string.format(question=q)\n",
    "    def process_ground_truth(ans):\n",
    "        return ans.split('\\n#### ')[1]\n",
    "    def process_prompt_completion(q, ans):\n",
    "        prompt = prompt_string.format(question=q)\n",
    "        cot =' ' + ans.split('\\n#### ')[0] + ' </think>'\n",
    "        gt = f\" <answer> {ans.split('\\n#### ')[1]} </answer>\"\n",
    "        return prompt + cot + gt\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        question_prompts = list(executor.map(process_question, ds[usage][\"question\"]))\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        ground_truth = list(executor.map(process_ground_truth, ds[usage][\"answer\"]))\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        prompt_completion = list(executor.map(process_prompt_completion, ds[usage][\"question\"], ds[usage][\"answer\"]))\n",
    "    return question_prompts, ground_truth, prompt_completion\n",
    "\n",
    "training_data = preprocess_dataset(ds, 'train')[2]\n",
    "test_prompt, test_gt =  preprocess_dataset(ds, 'test')[0], preprocess_dataset(ds, 'test')[1]\n",
    "train_ds = Dataset.from_dict({\"text\": preprocess_dataset(ds, 'train')[2]})\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"prompt\": test_prompt[:len(test_prompt)//2],\n",
    "    \"gt\": test_gt[:len(test_gt)//2],\n",
    "\n",
    "})\n",
    "test_ds = Dataset.from_dict({\n",
    "    \"prompt\": test_prompt[len(test_prompt)//2:],\n",
    "    \"gt\": test_gt[len(test_gt)//2:],\n",
    "\n",
    "})\n",
    "# val_ds = Dataset.from_dict({\n",
    "#     \"prompt\": test_prompt,\n",
    "#     \"gt\": test_gt,\n",
    "\n",
    "# })\n",
    "\n",
    "\n",
    "# Build a collator whose response_template matches your prompt ending\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer = tokenizer,\n",
    "    # Anything before *and including* this string gets label = -100\n",
    "    response_template  = r\"Assistant: <think>\",   # note the space after >\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "#preprocess_dataset(ds)[2][0:2]\n",
    "\n",
    "# some test on collator\n",
    "# curr_batch = tokenizer(\n",
    "#     preprocess_dataset(ds)[2][0:2],\n",
    "#     padding=False,               # collator can handle this , set to false\n",
    "#     truncation=False,           # collator will handle this with left truncate\n",
    "#     return_special_tokens_mask=False   # only need for MLM task\n",
    "#     #return_tensors=None        # lists, not tensors – collator wants lists\n",
    "# )\n",
    "\n",
    "# # #collator format [[dict] , [dict], [dict]]\n",
    "# cnt = len(curr_batch['input_ids'])\n",
    "\n",
    "# curr_batch_reconstruct =[]\n",
    "# for i in range(cnt):\n",
    "#     temp = dict()\n",
    "#     for k,vals in curr_batch.items():\n",
    "#         temp[k] = vals[i]\n",
    "#     curr_batch_reconstruct.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# collator_output_dict = collator.torch_call(curr_batch_reconstruct)\n",
    "# collator_output_dict['labels'][1]\n",
    "# print(collator_output_dict['input_ids'][0][128:])\n",
    "# print(collator_output_dict['labels'][0][128:])\n",
    "# print(len(collator_output_dict['input_ids'][0]))\n",
    "# print(len(collator_output_dict['labels'][0]))\n",
    "# # # print(tokenizer.decode(collator_output_dict['input_ids'][1]))\n",
    "# # # print(tokenizer.decode([42 if v==-100 else v for v in collator_output_dict['labels'][1].tolist() ]))\n",
    "\n",
    "\n",
    "# # #collator([12,33])\n",
    "# # #print(curr_batch.keys())\n",
    "# # #len(curr_batch['input_ids'][0]), len(curr_batch['input_ids'][1])    \n",
    "# # #curr_batch['attention_mask'][1] # the second mask has 1 digit 0\n",
    "tokenizer.save_pretrained('checkpoint/sft/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\\nUser: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\\nAssistant: <think>\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.decode([151645])\n",
    "#train_ds['text']\n",
    "tokenizer.pad_token_id\n",
    "#tokenizer.eos_token_id\n",
    "#val_ds['prompt']\n",
    "# print(training_data[0])\n",
    "#print(val_ds['prompt'][0])\n",
    "# print(ds['train']['question'][0])\n",
    "# print(ds['test']['question'][0])\n",
    "test_prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4736bc4ff5544112ac39ffb35331fb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5b96e0e3b0443f8e834710b423e358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e94d9a6ab324a718259c401c8fc4912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b311f75951d43dd8af3988a1b3b7796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhaoranyu66\u001b[0m (\u001b[33mudacity_jeff\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sagemaker-user/assignment5-alignment/cs336_alignment/wandb/run-20250609_015940-n7yxgvlt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/udacity_jeff/huggingface/runs/n7yxgvlt' target=\"_blank\">./checkpoint/sft/sft_lora_results3</a></strong> to <a href='https://wandb.ai/udacity_jeff/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/udacity_jeff/huggingface' target=\"_blank\">https://wandb.ai/udacity_jeff/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/udacity_jeff/huggingface/runs/n7yxgvlt' target=\"_blank\">https://wandb.ai/udacity_jeff/huggingface/runs/n7yxgvlt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='935' max='935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [935/935 16:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.524100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.467800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.458500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.459200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.417800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.434100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.373100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.374400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.417800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.339100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.370200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/myenv3/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.conda/envs/myenv3/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=935, training_loss=0.4155505194383509, metrics={'train_runtime': 970.02, 'train_samples_per_second': 7.704, 'train_steps_per_second': 0.964, 'total_flos': 2.267204337658368e+16, 'train_loss': 0.4155505194383509})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import numpy\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    max_seq_length=1024,\n",
    "    output_dir=\"./checkpoint/sft/sft_lora_results3\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-6,  \n",
    "    num_train_epochs=1,\n",
    "   \n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"],\n",
    "    warmup_ratio=0.05,\n",
    "    report_to = \"wandb\",  \n",
    "    bf16=True,   \n",
    "   \n",
    "    #pad_token_id=eos_id,\n",
    "    #eos_token_id=eos_id,          # <— this is what TRL will use to stop\n",
    "    # you can also set other generation defaults here if you like\n",
    ")\n",
    "\n",
    "\n",
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,  \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "   \n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    "    target_modules='all-linear'\n",
    "    #target_modules = [\"q_proj\",\"v_proj\"]\n",
    ")\n",
    "\n",
    "# 将LoRA配置应用到模型\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "#peft_model = PeftModel.from_pretrained(model, \"./checkpoint/sft/sft_lora_results3/checkpoint-500\", is_trainable=True )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 训练参数配置\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./checkpoint/sft/sft_lora_results\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     learning_rate=1e-4,  \n",
    "#     num_train_epochs=3,\n",
    "#     fp16=True,  \n",
    "#     logging_steps=1\n",
    "# )\n",
    "\n",
    "# 使用Trainer API进行训练\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "   \n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collator, \n",
    "    args=sft_config,\n",
    "   \n",
    "    \n",
    "    #data_collator=torch.utils.data.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "#with torch.serialization.safe_globals([numpy.core.multiarray._reconstruct]):\n",
    "\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "#trainer.train( resume_from_checkpoint=\"./checkpoint/sft/sft_lora_results3/checkpoint-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu124'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_vllm(hf_model_dir: str,  device: str, seed: int, gpu_memory_utilization: float = 0.85):\n",
    "    \"\"\"Start the inference process, here we use vLLM to hold a model on\n",
    "    a GPU separate from the policy.\n",
    "    \"\"\"\n",
    "    vllm_set_random_seed(seed)\n",
    "    # Monkeypatch from TRL:\n",
    "    # https://github.com/huggingface/trl/blob/\n",
    "    # 22759c820867c8659d00082ba8cf004e963873c1/trl/trainer/grpo_trainer.py\n",
    "    # Patch vLLM to make sure we can\n",
    "    # (1) place the vLLM model on the desired device (world_size_patch) and\n",
    "    # (2) avoid a test that is not designed for our setting (profiling_patch).\n",
    "    world_size_patch = patch(\"torch.distributed.get_world_size\", return_value=1)\n",
    "    profiling_patch = patch(\n",
    "    \"vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling\",\n",
    "    return_value=None\n",
    "    )\n",
    "    with world_size_patch, profiling_patch:\n",
    "        return LLM(\n",
    "        #model=\"Qwen/Qwen2.5-1.5B\",    # base model dir or HF name\n",
    "        #peft_model=\"./checkpoint/sft/sft_lora_results3/checkpoint-1500\",  # path to LoRA adapter\n",
    "        model=hf_model_dir,\n",
    "        #tokenizer=tokenizer,\n",
    "        #tokenizer_mode='auto',\n",
    "        device=device,\n",
    "        dtype=torch.bfloat16,\n",
    "        enable_prefix_caching=True,\n",
    "        gpu_memory_utilization=gpu_memory_utilization,\n",
    "        \n",
    "        )\n",
    "\n",
    "# def load_policy_into_vllm_instance(policy: PreTrainedModel, tokenizer, llm: LLM):\n",
    "#     \"\"\" Copied from https://github.com/huggingface/trl/blob/\n",
    "#     22759c820867c8659d00082ba8cf004e963873c1/trl/trainer/grpo_trainer.py#L670.\n",
    "#     \"\"\"\n",
    "#     state_dict = policy.state_dict()\n",
    "#     #llm_model = llm.llm_engine.model_runner.model\n",
    "#     def echo_model_runner(self):\n",
    "#         return self.model_runner.model.__class__\n",
    "#     llm_model = llm.collective_rpc(echo_model_runner)[0]\n",
    "#     #.model_executor.driver_worker.\n",
    "#     llm_model.load_weights(state_dict.items())\n",
    "#     #llm.llm_engine.tokenizer = tokenizer\n",
    "\n",
    "def sft_evaluation(hf_policy_dir: str, val_ds,  device: str, out_dir):\n",
    "    start_time = time.time()\n",
    "    #format_reward, answer_reward, answer = [], [], []\n",
    "    # total_response_len = 0\n",
    "    # total_response_len_correct = 0\n",
    "    # total_response_len_incorrect = 0\n",
    "    # total_sample = 0\n",
    "    # total_correct_sample = 0\n",
    "    # total_incorrect_sample = 0\n",
    "    response_avg_entropy_lst = [] #(batch,)\n",
    "    response_len_lst = [] #(batch,)\n",
    "    correct_lst = []\n",
    "    incorrect_lst = []\n",
    "    #initialize llm for vllm\n",
    "    llm = init_vllm(hf_policy_dir, device, 233)\n",
    "    #load_policy_into_vllm_instance(policy, tokenizer, llm)\n",
    "    sampling_params = SamplingParams(\n",
    "    temperature=1.0, top_p=1.0, max_tokens=1024, stop=[\"</answer>\"]\n",
    "    )\n",
    "    sampling_params.include_stop_str_in_output = True\n",
    "    all_prompt_texts = val_ds['prompt']\n",
    "    all_answer_gt = val_ds['gt']\n",
    "    # for batch in val_dataset:\n",
    "    #     all_prompt_texts.extend(batch['prompt_texts'])\n",
    "    #     all_answer_gt.extend(batch['answer_gt'] ) \n",
    "   \n",
    "    all_model_outputs = llm.generate(all_prompt_texts, sampling_params)\n",
    "    start2_time = time.time()\n",
    "    print(f'generate all response time: {start2_time - start_time}------------')\n",
    "\n",
    "    # still need policy model for eval mode\n",
    "    #policy.eval()\n",
    "    os.makedirs(out_dir.rsplit('/', 1)[0], exist_ok=True) \n",
    "    with open(out_dir, \"a\", encoding=\"utf-8\") as f:\n",
    "        format_reward, answer_reward, reward = 0, 0, 0\n",
    "        # for idx, batch in enumerate(tqdm(dataloader, total=len(dataloader))):\n",
    "        #     #start2_time = time.time()\n",
    "        #     #prompt_texts = batch['prompt_texts']\n",
    "        #     #answer_texts = batch['answer_texts'] \n",
    "        #     #answer_gt = batch['answer_gt'] \n",
    "            \n",
    "        #     #batch_size = batch['input_ids'].size(0)\n",
    "        #     input_ids = batch['input_ids'].to(policy_device)\n",
    "        #     labels = batch['labels'].to(policy_device)\n",
    "        #     response_masks = batch['response_mask'].to(policy_device)\n",
    "        #     with torch.no_grad():\n",
    "        #         response = get_response_log_probs(policy, input_ids, labels, True)\n",
    "        #         log_probs, token_entropy = response['log_probs'], response['token_entropy']\n",
    "        #         normalzied_token_entropy = token_entropy * response_masks\n",
    "                \n",
    "        #         curr_batch_response_len = torch.sum(response_masks,dim=1)#.tolist()\n",
    "        #         curr_batch_response_entropy = torch.sum(normalzied_token_entropy, dim=1)#.tolist()\n",
    "        #         curr_batch_response_avg_entropy = curr_batch_response_entropy / curr_batch_response_len\n",
    "                \n",
    "\n",
    "        #         curr_batch_response_len = curr_batch_response_len.tolist()\n",
    "        #         curr_batch_response_avg_entropy = curr_batch_response_avg_entropy.tolist()\n",
    "        #         response_len_lst.extend(curr_batch_response_len)\n",
    "        #         response_avg_entropy_lst.extend(curr_batch_response_avg_entropy)\n",
    "        #         # print('curr_batch_response_len', curr_batch_response_len)\n",
    "        #         # print('curr_batch_response_avg_entropy', curr_batch_response_avg_entropy)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        #     # # Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "        #     # # that contain the prompt, generated text, and other information.\n",
    "        #     # stop1_time = time.time()\n",
    "        #     # model_outputs = llm.generate(prompt_texts, sampling_params)\n",
    "        #     # stop2_time = time.time()\n",
    "\n",
    "        for i, (output, gt) in enumerate(zip(all_model_outputs, all_answer_gt)):\n",
    "            prompt = output.prompt\n",
    "            generated_text = output.outputs[0].text\n",
    "            \n",
    "            res = r1_zero_reward_fn(generated_text, gt)\n",
    "            # format_reward+= res['format_reward']\n",
    "            # answer_reward+= res['answer_reward']\n",
    "            # reward+= res['reward']\n",
    "            #print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}, format_reward: {str(res['format_reward'])}, answer_reward: {str(res['answer_reward'])}, reward: {str(res['reward'])}\")\n",
    "            #final_output.append([res['format_reward'], res['answer_reward'], res['reward']])\n",
    "            dp = {\n",
    "                \"prompt\": f\"{prompt}\",\n",
    "                \"ground_truth\": gt, \n",
    "                \"output\": f\"{generated_text}\",\n",
    "                \"format_reward\": res['format_reward'],\n",
    "                \"answer_reward\": res['answer_reward'],\n",
    "                \"reward\": res['reward'],\n",
    "                #\"avg_response_token_entropy\": response_avg_entropy_lst[i]\n",
    "            }\n",
    "            correct_lst.append(int(res['reward']==1))\n",
    "            incorrect_lst.append(int(res['reward']!=1))\n",
    "            format_reward+= res['format_reward']\n",
    "            answer_reward+= res['answer_reward']\n",
    "            reward+= res['reward']\n",
    "\n",
    "            json.dump(dp, f, ensure_ascii=False)\n",
    "            \n",
    "    #total_response_len_correct = np.sum(np.array(correct_lst) * np.array(response_len_lst))\n",
    "    #total_response_len_incorrect = np.sum(np.array(incorrect_lst) * np.array(response_len_lst))\n",
    "    total_correct_sample = np.sum(np.array(correct_lst))\n",
    "    total_incorrect_sample = np.sum(np.array(incorrect_lst))\n",
    "    total_sample = len(all_model_outputs)\n",
    "    #total_response_len = np.sum(np.array(response_len_lst))\n",
    "\n",
    "    print(f'total_correct_sample: {total_correct_sample}, total_incorrect_sample: {total_incorrect_sample}')\n",
    "    print(format_reward, answer_reward, reward)\n",
    "    #print(f'final stats\\navg_response_len={total_response_len/total_sample:.2f}, avg_response_len_correct={total_response_len_correct/total_correct_sample:.2f}, avg_response_len_incorrect={total_response_len_incorrect/total_incorrect_sample:.2f}')\n",
    "    #print(\"some check:\", total_response_len_correct+total_response_len_incorrect==total_response_len)\n",
    "    #print(\"moer check:\", total_correct_sample+total_incorrect_sample==total_sample)\n",
    "    import gc\n",
    "    del llm\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return total_correct_sample / total_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151666\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer_dir =  \"checkpoint/sft/tokenizer\"\n",
    "# orig_model_dir =      \"checkpoint/sft/sft_lora_results3/checkpoint-935/adapter_model.safetensors\"\n",
    "model_dir = \"checkpoint/sft/sft_lora_results3/checkpoint-935\"\n",
    "eval_out_dir = 'eval/sft/sft_lora_results3/checkpoint-935/sft_eval.jsonl'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     ).to(device)\n",
    "# # 2) Load only the adapter weights into the base model\n",
    "# adapter_sd = safetensors.torch.load_file(\n",
    "#     orig_model_dir,\n",
    "#     device=\"cpu\"\n",
    "# )\n",
    "# model.load_state_dict(adapter_sd, strict=False)\n",
    "tokenizer =  AutoTokenizer.from_pretrained(tokenizer_dir, local_files_only=True)\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "print(len(tokenizer))\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load LoRA adapter\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "\n",
    "# Merge LoRA weights into base model\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Save merged model and tokenizer in a new directory\n",
    "hf_model_dir = \"./checkpoint/sft/sft_lora_results3/my_merged_model\"\n",
    "merged_model.save_pretrained(hf_model_dir)\n",
    "tokenizer.save_pretrained(hf_model_dir)\n",
    "sft_evaluation(hf_model_dir, val_ds , device, eval_out_dir)\n",
    "\n",
    "#sft_evaluation(model, tokenizer_dir, val_ds, tokenizer_dir, device, device, eval_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-09 02:39:47 [config.py:793] This model supports multiple tasks: {'score', 'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-09 02:39:47 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-09 02:39:51 [__init__.py:243] Automatically detected platform cuda.\n",
      "INFO 06-09 02:39:54 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 06-09 02:39:54 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-09 02:39:54 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-09 02:39:54 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 06-09 02:39:54 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='Qwen/Qwen2.5-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 06-09 02:39:54 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f81a5b3e0f0>\n",
      "INFO 06-09 02:39:55 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-09 02:39:55 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-09 02:39:55 [gpu_model_runner.py:1531] Starting to load model Qwen/Qwen2.5-1.5B...\n",
      "INFO 06-09 02:39:55 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-09 02:39:55 [backends.py:35] Using InductorAdaptor\n",
      "INFO 06-09 02:39:55 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "INFO 06-09 02:39:55 [weight_utils.py:344] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.63it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-09 02:39:56 [default_loader.py:280] Loading weights took 0.48 seconds\n",
      "INFO 06-09 02:39:56 [gpu_model_runner.py:1549] Model loading took 2.9110 GiB and 0.907266 seconds\n",
      "INFO 06-09 02:40:02 [backends.py:459] Using cache directory: /home/sagemaker-user/.cache/vllm/torch_compile_cache/8d3851c0e6/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-09 02:40:02 [backends.py:469] Dynamo bytecode transform time: 5.96 s\n",
      "INFO 06-09 02:40:07 [backends.py:132] Directly load the compiled graph(s) for shape None from the cache, took 4.564 s\n",
      "INFO 06-09 02:40:08 [monitor.py:33] torch.compile takes 5.96 s in total\n",
      "INFO 06-09 02:40:09 [kv_cache_utils.py:637] GPU KV cache size: 396,928 tokens\n",
      "INFO 06-09 02:40:09 [kv_cache_utils.py:640] Maximum concurrency for 131,072 tokens per request: 3.03x\n",
      "INFO 06-09 02:40:28 [gpu_model_runner.py:1933] Graph capturing finished in 19 secs, took 0.45 GiB\n",
      "INFO 06-09 02:40:28 [core.py:167] init engine (profile, create kv cache, warmup model) took 31.62 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49384c644604d4a89921b75fbd39009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6328c66d5345d6931f2a430947192e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/659 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate all response time: 25.8168466091156------------\n",
      "total_correct_sample: 26, total_incorrect_sample: 633\n",
      "163.0 26.0 26.0\n"
     ]
    }
   ],
   "source": [
    "# base model evaluation\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "eval_out_dir = 'eval/sft/sft_lora_results3/base_model/sft_eval.jsonl'\n",
    "llm = LLM(model_name, dtype=\"bfloat16\", gpu_memory_utilization=0.85)\n",
    "# Create a sampling params object, stopping generation on newline.\n",
    "sampling_params = SamplingParams(\n",
    "temperature=1.0, top_p=1.0, max_tokens=1024, stop=[\"</answer>\"]\n",
    ")\n",
    "sampling_params.include_stop_str_in_output = True\n",
    "start_time = time.time()\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "\n",
    "all_prompt_texts = val_ds['prompt']\n",
    "all_answer_gt = val_ds['gt']\n",
    "\n",
    "\n",
    "all_model_outputs = llm.generate(all_prompt_texts, sampling_params)\n",
    "start2_time = time.time()\n",
    "print(f'generate all response time: {start2_time - start_time}------------')\n",
    "correct_lst = []\n",
    "incorrect_lst = []\n",
    "# still need policy model for eval mode\n",
    "#policy.eval()\n",
    "os.makedirs(eval_out_dir.rsplit('/', 1)[0], exist_ok=True) \n",
    "with open(eval_out_dir, \"a\", encoding=\"utf-8\") as f:\n",
    "    format_reward, answer_reward, reward = 0, 0, 0\n",
    "    for i, (output, gt) in enumerate(zip(all_model_outputs, all_answer_gt)):\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        \n",
    "        res = r1_zero_reward_fn(generated_text, gt)\n",
    "        # format_reward+= res['format_reward']\n",
    "        # answer_reward+= res['answer_reward']\n",
    "        # reward+= res['reward']\n",
    "        #print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}, format_reward: {str(res['format_reward'])}, answer_reward: {str(res['answer_reward'])}, reward: {str(res['reward'])}\")\n",
    "        #final_output.append([res['format_reward'], res['answer_reward'], res['reward']])\n",
    "        dp = {\n",
    "            \"prompt\": f\"{prompt}\",\n",
    "            \"ground_truth\": gt, \n",
    "            \"output\": f\"{generated_text}\",\n",
    "            \"format_reward\": res['format_reward'],\n",
    "            \"answer_reward\": res['answer_reward'],\n",
    "            \"reward\": res['reward'],\n",
    "            #\"avg_response_token_entropy\": response_avg_entropy_lst[i]\n",
    "        }\n",
    "        correct_lst.append(int(res['reward']==1))\n",
    "        incorrect_lst.append(int(res['reward']!=1))\n",
    "        format_reward+= res['format_reward']\n",
    "        answer_reward+= res['answer_reward']\n",
    "        reward+= res['reward']\n",
    "\n",
    "        json.dump(dp, f, ensure_ascii=False)\n",
    "            \n",
    "#total_response_len_correct = np.sum(np.array(correct_lst) * np.array(response_len_lst))\n",
    "#total_response_len_incorrect = np.sum(np.array(incorrect_lst) * np.array(response_len_lst))\n",
    "total_correct_sample = np.sum(np.array(correct_lst))\n",
    "total_incorrect_sample = np.sum(np.array(incorrect_lst))\n",
    "total_sample = len(all_model_outputs)\n",
    "#total_response_len = np.sum(np.array(response_len_lst))\n",
    "\n",
    "print(f'total_correct_sample: {total_correct_sample}, total_incorrect_sample: {total_incorrect_sample}')\n",
    "print(format_reward, answer_reward, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'72'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output = \" We'll use cancellation technique to solve this problem. Natalia sold half as many clips in may as she had in April. That means she sold 12 clips in May since 48 / 2 = 24. Note that we must never cancel the first 24/, as it is 0. Our answer is 48+24 = 72 clips.</think>\\n\\n<answer> 72 </answer>\"\n",
    "# r1_zero_reward_fn(output, '72')\n",
    "\n",
    "all_prompt_texts = val_ds['prompt']\n",
    "all_prompt_texts[0]\n",
    "val_ds['gt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,  11657,   6236,   6331,\n",
       "            879,   2677,  30580,    304,    279,   1707,    315,    264,  53966,\n",
       "         151645,    198, 151644,    872,    198,   4340,   1657,  58332,    646,\n",
       "            264,   3738,   8180,    304,    825,  11699,     30, 151645,    198,\n",
       "         151644,  77091,    198]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#tokenizer.decode(tokenizer.pad_token_id)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",},\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "tokenized_chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a friendly chatbot who always responds in the style of a pirate<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_chat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']\n"
     ]
    }
   ],
   "source": [
    "#setup_chat_format is used to create chat template with [{role:xx, content:xx}] data, to convert it to purely text\n",
    "#we dont need to setup_chat_format for qwen. it has been setup\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from trl import SFTConfig, SFTTrainer\n",
    "# from trl import setup_chat_format\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "# tokenizer.decode(tokenizer.eos_token_id)\n",
    "print(tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
